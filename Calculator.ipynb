{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary\n",
    "chars = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"+\", \"=\", \".\"]\n",
    "\n",
    "# Tokenization\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(chars)\n",
    "MAX_NUMBER = 9 # the equations will have numbers from [0 - MAX_NUMBER]\n",
    "EMBEDDING_SIZE = 16\n",
    "CONTEXT_SIZE = len(str(MAX_NUMBER))*2 + len(str(MAX_NUMBER+MAX_NUMBER)) + 2 # context window size just big enough to always see the whole equation\n",
    "BATCH_SIZE = 64\n",
    "MAX_STEPS = 5000\n",
    "LEARNING_RATE = 3E-4\n",
    "BLOCK_COUNT = 5\n",
    "NUM_HEADS = 6\n",
    "DROPOUT = 0.2\n",
    "HEAD_SIZE = EMBEDDING_SIZE // NUM_HEADS # How big Query, Key and Value matrices are\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "EVAL_INTERVAL = 500\n",
    "EVAL_LOSS_BATCHES = 200\n",
    "\n",
    "this_model_name = \"model_EX1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader that returns a batch of equations: label=\"a+b=\", target=\"c\"\n",
    "# Start with numbers 0-9 @TODO: Increase to larger numbers\n",
    "def get_batch():\n",
    "    a = torch.randint(0, MAX_NUMBER+1, (BATCH_SIZE, ))\n",
    "    b = torch.randint(0, MAX_NUMBER+1, (BATCH_SIZE, ))\n",
    "\n",
    "    equations = [f\"{it1}+{it2}={it1+it2}\" for it1, it2 in zip(a.tolist(), b.tolist())]\n",
    "    equations = [eq + \".\"*(CONTEXT_SIZE - len(eq)) for eq in equations] # pad with \".\" at the end of equation to fill CONTEXT_SIZE\n",
    "\n",
    "    x = torch.tensor([encode(eq) for eq in equations])\n",
    "    y = torch.tensor([encode(eq[1:] + \".\") for eq in equations])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6]) torch.Size([64, 6])\n",
      "tensor([ 0, 10,  7, 11,  7, 12]) tensor([10,  7, 11,  7, 12, 12])\n",
      "\"0\" => \"+\"\n",
      "\"0+\" => \"7\"\n",
      "\"0+7\" => \"=\"\n",
      "\"0+7=\" => \"7\"\n",
      "\"0+7=7\" => \".\"\n",
      "\"0+7=7.\" => \".\"\n"
     ]
    }
   ],
   "source": [
    "# SHOW FIRST EXAMPLE OF BATCH\n",
    "xb, yb = get_batch()\n",
    "print(xb.shape, yb.shape)\n",
    "print(xb[0], yb[0])\n",
    "for i in range(CONTEXT_SIZE):\n",
    "    labels = decode(xb[0][:i+1].tolist())\n",
    "    target = itos[yb[0][i].item()]\n",
    "    print(f'\"{labels}\" => \"{target}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Multiple Heads of Self-Attention that are processed in parallel \"\"\"\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Single Heads in parallel\n",
    "        self.query = torch.randn([num_heads, EMBEDDING_SIZE, head_size]) * 0.02\n",
    "        self.key = torch.randn([num_heads, EMBEDDING_SIZE, head_size]) * 0.02\n",
    "        self.value = torch.randn([num_heads, EMBEDDING_SIZE, head_size]) * 0.02\n",
    "\n",
    "        self.dropout1 = nn.Dropout(DROPOUT)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(CONTEXT_SIZE, CONTEXT_SIZE)))\n",
    "        \n",
    "        # Only For Multi Head\n",
    "        self.proj = nn.Linear(num_heads*head_size, EMBEDDING_SIZE) # back to original size (see 3b1b Value↑ matrix)\n",
    "        self.dropout2 = nn.Dropout(DROPOUT)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n_batch, n_context, n_emb = x.shape\n",
    "        num_heads, head_size = self.query.shape[0], self.query.shape[-1]\n",
    "\n",
    "        # (num_heads, n_batch, n_context, head_size)\n",
    "        q = torch.einsum('bxy,iyk->bxik', (x, self.query)).view(num_heads, n_batch, n_context, head_size)\n",
    "        k = torch.einsum('bxy,iyk->bxik', (x, self.key)).view(num_heads, n_batch, n_context, head_size)\n",
    "        v = torch.einsum('bxy,iyk->bxik', (x, self.value)).view(num_heads, n_batch, n_context, head_size)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * q.shape[-1]**-0.5 # (num_heads, n_batch, n_context, n_context)\n",
    "        wei = wei.masked_fill(self.tril[:n_context, :n_context] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (num_heads, n_batch, n_context, n_context)\n",
    "        wei = self.dropout1(wei)\n",
    "\n",
    "        self.out = wei @ v # (num_heads, n_batch, n_context, head_size)\n",
    "        self.out = self.out.view(n_batch, n_context, num_heads*head_size)\n",
    "        self.out = self.dropout2(self.proj(self.out)) # (n_batch, n_context, EMBEDDING_SIZE)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_feat):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_feat, in_feat * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * in_feat, in_feat),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block: Communication (MultiHead Attention) followed by computation (MLP - FeedForward)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.sa_heads = CausalSelfAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(EMBEDDING_SIZE)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "        self.ln2 = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x + because their are residual connections around Masked Multi-Head Attention and Feed Forward (see Transformer Architecture)\n",
    "        x = x + self.sa_heads(self.ln1(x)) # (BATCH_SIZE, CONTEXT_SIZE, num_heads*head_size)\n",
    "        x = x + self.ffwd(self.ln2(x)) # (BATCH_SIZE, CONTEXT_SIZE, num_heads*head_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "             -inf,    -inf,  1.0000,    -inf,    -inf],\n",
       "         [ 1.0000,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "             -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "         [   -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
       "             -inf,    -inf,    -inf,  1.0000,    -inf],\n",
       "         [-2.1674,  1.9481,  0.2714,  0.0994, -0.5013, -2.3173, -0.6925,  0.1254,\n",
       "           1.2330,  0.4711,  0.9592,  0.3126, -0.2099],\n",
       "         [ 0.4638,  0.3835,  0.3073,  0.4802, -0.1251,  1.0787,  0.3154, -0.9656,\n",
       "          -1.3152,  0.9500,  0.2395,  0.8869,  0.2441],\n",
       "         [-0.3549, -1.0478, -3.2440,  1.1116,  0.1357, -1.0144, -1.3654,  0.9175,\n",
       "           1.2289,  0.3988,  0.8242,  1.7663,  0.0610]]),\n",
       " tensor([10,  0, 11,  3, 12, 12]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Masking out tokens from input tokens a+b= for Loss Calculation.\n",
    "The model should not receive a loss for mispredicting the next token before the \"=\"(stoi[\"=\"]=11) sign, since it can only guess here.\n",
    "Achieve by setting logits to the targets for input tokens.\n",
    "\n",
    "\"8\" => \"+\"      | IGNORE\n",
    "\"8+\" => \"2\"     | IGNORE\n",
    "\"8+2\" => \"=\"    | IGNORE\n",
    "\"8+2=\" => \"1\"   | USE\n",
    "\"8+2=1\" => \"0\"  | USE\n",
    "\"8+2=10\" => \".\" | USE\n",
    "\"\"\"\n",
    "\n",
    "xb, yb = get_batch()\n",
    "logits = torch.randn((BATCH_SIZE, CONTEXT_SIZE, VOCAB_SIZE))\n",
    "\n",
    "# Step 1) Create correct logits from targets\n",
    "correct_logits = torch.zeros_like(logits).scatter_(2, yb.unsqueeze(2), 1)\n",
    "correct_logits[correct_logits == 0] = float('-inf')\n",
    "\n",
    "# Step 2) For each item in batch, find out at which index in Context the \"=\" is\n",
    "equal_idx = (yb == stoi[\"=\"]).nonzero()[:, 1]\n",
    "\n",
    "# Step 3) Replace logits up until \"=\" with correct_logits without using a loop\n",
    "mask = torch.arange(CONTEXT_SIZE).unsqueeze(0) <= equal_idx.unsqueeze(1)\n",
    "logits = torch.where(mask.unsqueeze(2), correct_logits, logits)\n",
    "\n",
    "logits[0], yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # add an Embedding Table for Character Embedding\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        self.position_embedding_table = nn.Embedding(CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        self.blocks = nn.Sequential(*[Block(NUM_HEADS, HEAD_SIZE) for _ in range(BLOCK_COUNT)])\n",
    "        self.ln_f = nn.LayerNorm(EMBEDDING_SIZE) # final layer norm\n",
    "        self.lm_head = nn.Linear(EMBEDDING_SIZE, VOCAB_SIZE)\n",
    "\n",
    "        # better initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        n_batch, n_context = x.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(x) # (BATCH_SIZE, CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(0, n_context, device=device)) # position embedding for each char in CONTEXT (CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        x = tok_emb + pos_emb # (BATCH_SIZE, CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x) # (BATCH_SIZE, CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        logits = self.lm_head(x) # (BATCH_SIZE, CONTEXT_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # all tokens before '=' should not have impact on loss since model can only guess here\n",
    "            logits = self._mask_out_input_tokens(logits, y)\n",
    "\n",
    "            logits = logits.view(n_batch*n_context, VOCAB_SIZE)\n",
    "            y = y.view(n_batch*CONTEXT_SIZE)\n",
    "\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # See Code Cell above for more detailed explanation\n",
    "    def _mask_out_input_tokens(self, logits, y):\n",
    "        correct_logits = torch.zeros_like(logits).scatter_(2, y.unsqueeze(2), 1)\n",
    "        correct_logits[correct_logits == 0] = float('-inf')\n",
    "        equal_idx = (y == stoi[\"=\"]).nonzero()[:, 1]\n",
    "        mask = torch.arange(CONTEXT_SIZE).unsqueeze(0) <= equal_idx.unsqueeze(1)\n",
    "        logits = torch.where(mask.unsqueeze(2), correct_logits, logits)\n",
    "        return logits\n",
    "\n",
    "    def calculate(self, equation):\n",
    "        assert isinstance(equation, str), \"The variable 'equation' must be a string\"\n",
    "        assert re.match(r'^\\d+\\+\\d+=', equation), \"Equation must be of shape 'a+b='\"\n",
    "        a, b = equation[:-1].split(\"+\") \n",
    "        assert 0 <= int(a) <= MAX_NUMBER and 0 <= int(b) <= MAX_NUMBER, f\"The variables must be in [0, {MAX_NUMBER}]\"\n",
    "        \n",
    "        output = torch.tensor(encode(equation), device=device)\n",
    "        while output[-1] != stoi[\".\"] and len(output) <= CONTEXT_SIZE:\n",
    "            try:\n",
    "                logits, _ = self(output.view(1, -1))\n",
    "            except IndexError as err:\n",
    "                print(output.tolist())\n",
    "                \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_digit = torch.multinomial(probs[0, -1], num_samples=1)\n",
    "            output = torch.cat((output, next_digit))\n",
    "\n",
    "        if output[-1] != stoi[\".\"]:\n",
    "            return equation, -1\n",
    "\n",
    "        equation_answer = decode(output[:-1].tolist())\n",
    "        answer = equation_answer.replace(equation, \"\")\n",
    "        answer = int(answer) if answer.isdigit() else -1\n",
    "        \n",
    "        return equation, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean loss for {EVAL_LOSS_BATCHES}x batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(EVAL_LOSS_BATCHES, device=device)\n",
    "    for i in range(EVAL_LOSS_BATCHES):\n",
    "        X, Y = get_batch()\n",
    "        _, loss = model(X, Y)\n",
    "        losses[i] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/5000 [00:01<10:34,  7.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000) Loss: 1.2810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 510/5000 [00:11<03:50, 19.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500/5000) Loss: 0.5256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1006/5000 [00:23<05:28, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000/5000) Loss: 0.4830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1506/5000 [00:36<04:34, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500/5000) Loss: 0.4741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2006/5000 [00:48<03:51, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000/5000) Loss: 0.4700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2508/5000 [01:00<03:19, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500/5000) Loss: 0.4678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3009/5000 [01:13<02:36, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000/5000) Loss: 0.4682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3505/5000 [01:25<02:30,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500/5000) Loss: 0.4675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4007/5000 [01:38<01:13, 13.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000/5000) Loss: 0.4662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4506/5000 [01:50<00:40, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4500/5000) Loss: 0.4669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:02<00:00, 40.78it/s]\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for step in tqdm(range(MAX_STEPS)):\n",
    "    # calculate loss every once in a while\n",
    "    if step % EVAL_INTERVAL == 0:\n",
    "        loss = estimate_loss(model)\n",
    "        print(f\"Step {step}/{MAX_STEPS}) Loss: {loss:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5+0=', 8)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference (solve simple Addition Equations)\n",
    "model.eval()\n",
    "output = model.calculate(\"5+0=\")\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 117.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy: 7.1%'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Accuaracy\n",
    "correct = 0\n",
    "max = 1000\n",
    "for _ in tqdm(range(max)):\n",
    "    a, b = torch.randint(0, MAX_NUMBER, (2, )).tolist()\n",
    "    eq, c = model.calculate(f\"{a}+{b}=\")\n",
    "        \n",
    "    if c == a+b:\n",
    "        correct += 1\n",
    "\n",
    "f\"Accuracy: {round(correct/max*100, 2)}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Loss so good and Accuracy so low??\n",
    "The Loss only seems really good, since all the input tokens (\"a+b=\") are always copied from the targets. This means the model seems to have guessed most values right, although they are only copied and overwritten. In Conclusion: This model still sucks at arithmetic!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
