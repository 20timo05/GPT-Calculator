{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vocabulary\n",
    "chars = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"+\", \"=\", \".\"]\n",
    "\n",
    "# Tokenization\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(chars)\n",
    "MAX_NUMBER = 9 # the equations will have numbers from [0 - MAX_NUMBER]\n",
    "EMBEDDING_SIZE = 32\n",
    "CONTEXT_SIZE = len(str(MAX_NUMBER))*2 + len(str(MAX_NUMBER+MAX_NUMBER)) + 2 # context window size just big enough to always see the whole equation\n",
    "BATCH_SIZE = 64\n",
    "MAX_STEPS = 5000\n",
    "LEARNING_RATE = 3E-4\n",
    "BLOCK_COUNT = 2\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.2\n",
    "HEAD_SIZE = EMBEDDING_SIZE // NUM_HEADS # How big Query, Key and Value matrices are\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "EVAL_INTERVAL = 500\n",
    "EVAL_LOSS_BATCHES = 200\n",
    "\n",
    "this_model_name = \"model_EX1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader that returns a batch of equations: label=\"a+b=\", target=\"c\"\n",
    "# Start with numbers 0-9 @TODO: Increase to larger numbers\n",
    "def get_batch():\n",
    "    a = torch.randint(0, MAX_NUMBER+1, (BATCH_SIZE, ))\n",
    "    b = torch.randint(0, MAX_NUMBER+1, (BATCH_SIZE, ))\n",
    "\n",
    "    equations = [f\"{it1}+{it2}={it1+it2}\" for it1, it2 in zip(a.tolist(), b.tolist())]\n",
    "    equations = [eq + \".\"*(CONTEXT_SIZE - len(eq)) for eq in equations] # pad with \".\" at the end of equation to fill CONTEXT_SIZE\n",
    "\n",
    "    x = torch.tensor([encode(eq) for eq in equations])\n",
    "    y = torch.tensor([encode(eq[1:] + \".\") for eq in equations])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6]) torch.Size([64, 6])\n",
      "tensor([ 3, 10,  6, 11,  9, 12]) tensor([10,  6, 11,  9, 12, 12])\n",
      "3 => +\n",
      "3+ => 6\n",
      "3+6 => =\n",
      "3+6= => 9\n",
      "3+6=9 => .\n",
      "3+6=9. => .\n"
     ]
    }
   ],
   "source": [
    "# SHOW FIRST EXAMPLE OF BATCH\n",
    "xb, yb = get_batch()\n",
    "print(xb.shape, yb.shape)\n",
    "print(xb[0], yb[0])\n",
    "for i in range(CONTEXT_SIZE):\n",
    "    labels = decode(xb[0][:i+1].tolist())\n",
    "    target = itos[yb[0][i].item()]\n",
    "    print(f\"{labels} => {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Multiple Heads of Self-Attention that are processed in parallel \"\"\"\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Single Heads in parallel\n",
    "        self.query = torch.randn([num_heads, EMBEDDING_SIZE, head_size]) * 0.02\n",
    "        self.key = torch.randn([num_heads, EMBEDDING_SIZE, head_size]) * 0.02\n",
    "        self.value = torch.randn([num_heads, EMBEDDING_SIZE, head_size]) * 0.02\n",
    "\n",
    "        self.dropout1 = nn.Dropout(DROPOUT)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(CONTEXT_SIZE, CONTEXT_SIZE)))\n",
    "        \n",
    "        # Only For Multi Head\n",
    "        self.proj = nn.Linear(num_heads*head_size, EMBEDDING_SIZE) # back to original size (see 3b1b Value↑ matrix)\n",
    "        self.dropout2 = nn.Dropout(DROPOUT)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n_batch, n_context, n_emb = x.shape\n",
    "        num_heads, head_size = self.query.shape[0], self.query.shape[-1]\n",
    "\n",
    "        # (num_heads, n_batch, n_context, head_size)\n",
    "        q = torch.einsum('bxy,iyk->bxik', (x, self.query)).view(num_heads, n_batch, n_context, head_size)\n",
    "        k = torch.einsum('bxy,iyk->bxik', (x, self.key)).view(num_heads, n_batch, n_context, head_size)\n",
    "        v = torch.einsum('bxy,iyk->bxik', (x, self.value)).view(num_heads, n_batch, n_context, head_size)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * q.shape[-1]**-0.5 # (num_heads, n_batch, n_context, n_context)\n",
    "        wei = wei.masked_fill(self.tril[:n_context, :n_context] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (num_heads, n_batch, n_context, n_context)\n",
    "        wei = self.dropout1(wei)\n",
    "\n",
    "        self.out = wei @ v # (num_heads, n_batch, n_context, head_size)\n",
    "        self.out = self.out.view(n_batch, n_context, num_heads*head_size)\n",
    "        self.out = self.dropout2(self.proj(self.out)) # (n_batch, n_context, EMBEDDING_SIZE)\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_feat):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_feat, in_feat * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * in_feat, in_feat),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block: Communication (MultiHead Attention) followed by computation (MLP - FeedForward)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.sa_heads = CausalSelfAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(EMBEDDING_SIZE)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "        self.ln2 = nn.LayerNorm(EMBEDDING_SIZE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x + because their are residual connections around Masked Multi-Head Attention and Feed Forward (see Transformer Architecture)\n",
    "        x = x + self.sa_heads(self.ln1(x)) # (BATCH_SIZE, CONTEXT_SIZE, num_heads*head_size)\n",
    "        x = x + self.ffwd(self.ln2(x)) # (BATCH_SIZE, CONTEXT_SIZE, num_heads*head_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # add an Embedding Table for Character Embedding\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "        self.position_embedding_table = nn.Embedding(CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        self.blocks = nn.Sequential(*[Block(NUM_HEADS, HEAD_SIZE) for _ in range(BLOCK_COUNT)])\n",
    "        self.ln_f = nn.LayerNorm(EMBEDDING_SIZE) # final layer norm\n",
    "        self.lm_head = nn.Linear(EMBEDDING_SIZE, VOCAB_SIZE)\n",
    "\n",
    "        # better initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        n_batch, n_context = x.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(x) # (BATCH_SIZE, CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(0, n_context, device=device)) # position embedding for each char in CONTEXT (CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        x = tok_emb + pos_emb # (BATCH_SIZE, CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x) # (BATCH_SIZE, CONTEXT_SIZE, EMBEDDING_SIZE)\n",
    "        logits = self.lm_head(x) # (BATCH_SIZE, CONTEXT_SIZE, VOCAB_SIZE)\n",
    "        \n",
    "        if y is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(n_batch*n_context, VOCAB_SIZE)\n",
    "            y = y.view(n_batch*CONTEXT_SIZE)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, previous_text, max_new_tokens):\n",
    "        output = previous_text\n",
    "        for _ in tqdm(range(max_new_tokens)):\n",
    "            last_tokens = torch.tensor(encode(output[-CONTEXT_SIZE:]), device=device)\n",
    "            \n",
    "            # add batch dimension and feed to model\n",
    "            logits, _ = self(last_tokens.view(1, -1))\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            probs_next_char = probs[0, -1]\n",
    "            new_char = itos[torch.multinomial(probs_next_char, num_samples=1).item()]\n",
    "\n",
    "            output += new_char\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean loss for {EVAL_LOSS_BATCHES}x batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(EVAL_LOSS_BATCHES, device=device)\n",
    "        for i in tqdm(range(EVAL_LOSS_BATCHES)):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 49.09it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.54it/s]\n",
      "  0%|          | 4/5000 [00:07<1:58:07,  1.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/5000) train: 4.4307, val: 4.4299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 66.95it/s]]\n",
      "100%|██████████| 200/200 [00:03<00:00, 60.14it/s]\n",
      " 10%|█         | 506/5000 [00:27<26:08,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500/5000) train: 2.6391, val: 2.6246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 55.95it/s]]\n",
      "100%|██████████| 200/200 [00:03<00:00, 59.50it/s]\n",
      " 20%|██        | 1005/5000 [00:49<28:28,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000/5000) train: 2.4151, val: 2.4116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 59.41it/s]s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 61.39it/s]\n",
      " 30%|███       | 1504/5000 [01:10<31:29,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500/5000) train: 2.3192, val: 2.2993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 32.96it/s]s]\n",
      "100%|██████████| 200/200 [00:04<00:00, 43.40it/s]\n",
      " 40%|████      | 2004/5000 [01:48<58:50,  1.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000/5000) train: 2.2506, val: 2.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 26.94it/s]s]\n",
      "100%|██████████| 200/200 [00:07<00:00, 26.79it/s]s]\n",
      " 50%|█████     | 2501/5000 [02:37<1:36:19,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500/5000) train: 2.2076, val: 2.2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 62.50it/s]s]  \n",
      "100%|██████████| 200/200 [00:03<00:00, 60.29it/s]\n",
      " 60%|██████    | 3006/5000 [03:01<11:54,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000/5000) train: 2.1923, val: 2.1983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 63.92it/s]s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 70.99it/s]\n",
      " 70%|███████   | 3504/5000 [03:24<12:42,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500/5000) train: 2.1726, val: 2.1736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:03<00:00, 57.80it/s]s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 54.01it/s]\n",
      " 80%|████████  | 4004/5000 [03:48<07:11,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000/5000) train: 2.1571, val: 2.1518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 67.03it/s]s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 59.63it/s]\n",
      " 90%|█████████ | 4504/5000 [04:11<03:15,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4500/5000) train: 2.1410, val: 2.1442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [04:28<00:00, 18.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model = GPT()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for step in tqdm(range(MAX_STEPS)):\n",
    "    # calculate loss every once in a while\n",
    "    if step % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {step}/{MAX_STEPS}) train: {losses['train']:.4f}, val: {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 216.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ton as oves at chat lidnize nitts be this it his. PenumnaeMe andilm cione ceeart's\n",
      "Dery agearmben't mup the -fop a havet hand bardourp’t dic Verneare Andss he men the moss trely\n",
      "of the whe abe a noir mugh das at havery; he spalein was bad. A blemp of Crimsed. There’s'n Harry, it she noartedortpimedfonsed, suingly one, wald Ron.\n",
      "Harr; Dongefivell whis and but sils?”\n",
      "\"Sid nus ove\n",
      "wor\n",
      "bibe Prublet in. S, saigho bloplayblainbreathes; so?A pepthe boorey cumbrice wroundlou coven, jabcenuc of as closing arry. They edorich romts,beampolly and\n",
      "the, leired geee Maen he mi,\n",
      "sMe thand he thered's. Wealld yowad lik awed you're said notathe a\n",
      "got mapt, his beds, wexpblatathand the stalfoake coking the owakerstiss,\n",
      "lition at at omian. The gaming of and wharme.. Harry bund I to chiy offfotter.\n",
      "This te whe; yon to Me waslos uing k.” . ., Bec, harry pead the theing it,” said Sno?”\n",
      "\"Dunat and Tung could\u0002torned Mlyowly. I\n",
      "pide said\n",
      "HoAI geced for wich seaimsbelted exere a He fesiled andy”\n",
      "“Nast wald parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference (Generate Harry Potter'ish text)\n",
    "model.eval()\n",
    "output = model.generate(\"\\n\", 1000)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
